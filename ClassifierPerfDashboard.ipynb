{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Binary Classifier Performance Dashboard</center></h1>\n",
    "\n",
    "\n",
    "[Binary Classifiers](https://en.wikipedia.org/wiki/Binary_classification) are widely used for various tasks in ML & Data Science. Regardless of the choice of ML algorithms (Logistic, Tree based or Neural Nets), a binary classifiers performance is often optimized or measured based on a select few metrics like AUC-ROC, F-score, etc. \n",
    "\n",
    "This App is an attempt to allow ML/DS practitioners more time to understand their results by the way of\n",
    "- Automating the process of creating some of the popular metrics\n",
    "- Creating an informative visual dashboard to aid optimal threshold selection\n",
    "\n",
    "**Metrics of interest in this App**\n",
    "- AUC-ROC curve & AUC\n",
    "- N-tile (x-axis) V Target rate (Y-axis) charts - useful viz for how well your model slopes target\n",
    "- Best possible F-scores and the corresponding threshold values and confusion matrices\n",
    "- For a threshold of users choosing F-scores and the confusion matrix\n",
    "\n",
    "\n",
    "    \n",
    "<h3><center>Directions For Use:</center></h3>   \n",
    "\n",
    "    \n",
    "1. Upload a csv with two columns- target variable and model predictions\n",
    "2. You will see an AUC-ROC curve & the AUC measure\n",
    "3. Select the number of buckets(n-tile) using the slider for the N-tile V Target rate chart\n",
    "4. Row 2 conatins the best possible F-scores and corresponding thresholds, confusion matrices. \n",
    "5. Adjust the slider to a threshold of your choosing to create the corresponding metrics - precision, recall and confusion matrix.\n",
    "\n",
    "$\\color{#f03c15}{**Note**}$: Column names must be target and preds, the target variable must be dichotomous only taking the values 0 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the req libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, recall_score, precision_score\n",
    "import io\n",
    "from ipywidgets import VBox, HBox, Layout\n",
    "\n",
    "## graphing tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "from  matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##upload button - for the dataset containing target and model prediction\n",
    "btn_upload = widgets.FileUpload(accept = 'text/csv'\n",
    "                                ,align_items='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## slider for Threshold\n",
    "sldr = widgets.FloatSlider(value=0.500\n",
    "                           , min=0.000\n",
    "                           , max=1.000\n",
    "                           , step=0.001\n",
    "                           , description='Threshold:'\n",
    "                           , disabled=False\n",
    "                           , continuous_update=True\n",
    "                           , orientation='horizontal'\n",
    "                           , align_items='center'\n",
    "                           , readout=True\n",
    "                           , readout_format='.3f')\n",
    "## slider for n-tiles\n",
    "ntile_sldr = widgets.IntSlider(value=10\n",
    "                               , min=5\n",
    "                               , max=25\n",
    "                               , step=5\n",
    "                               , description='N-tiles:'\n",
    "                               , disabled=False\n",
    "                               , continuous_update=True\n",
    "                               , orientation='horizontal'\n",
    "                               , align_items='center'\n",
    "                               , readout=True\n",
    "                               , readout_format='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = widgets.Output()\n",
    "out1 = widgets.Output()\n",
    "out2 = widgets.Output()\n",
    "out3 = widgets.Output()\n",
    "out4 = widgets.Output()\n",
    "out5 = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "lbl_pred1 = widgets.Label()\n",
    "lbl_pred2 = widgets.Label()\n",
    "lbl_predPoint5 = widgets.Label()\n",
    "lbl_pred1Tr = widgets.Label()\n",
    "lbl_pred2Tr = widgets.Label()\n",
    "lbl_predPoint5Tr = widgets.Label()\n",
    "lbl_user_precision = widgets.Label()\n",
    "lbl_user_recall = widgets.Label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper func for creating n-tiles\n",
    "def true_ntile_dist(x_data, ntile = 10):\n",
    "    \n",
    "    ## model scores on the dataset you want to build the ntiles off of\n",
    "    score = x_data['preds']\n",
    "    \n",
    "    ## ntile calculation\n",
    "    ntile_list = [0]\n",
    "    nvar = int(100/ntile)\n",
    "    for i in range(1,ntile+1):\n",
    "        a = (np.percentile(score, nvar*i))\n",
    "        ntile_list.insert(len(ntile_list),a)\n",
    "       \n",
    "    ntile_list[-1] = 1\n",
    "    \n",
    "    ## creating the empty dataframe\n",
    "    df_ntiled = x_data\n",
    "    try :\n",
    "        df_ntiled['N-tile buckets'] = pd.cut(score, ntile_list, labels = [i for i in range(1,ntile+1)])\n",
    "        df = df_ntiled[['N-tile buckets','target']].groupby('N-tile buckets').mean().reset_index()\n",
    "        df['Target Rate'] = df['target']\n",
    "    except ValueError:\n",
    "        print('Error: Probably exceeded the number of bins possible on your data')\n",
    "        \n",
    "    ## return the deciled dataframe and the decile cuts     \n",
    "    return df, ntile_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the onclick event for upload button\n",
    "def on_click(change):\n",
    "    ## read in the upload as pandas dataframe\n",
    "    tabl = pd.read_csv(io.BytesIO(btn_upload.data[-1]))\n",
    "    \n",
    "    ## clear output\n",
    "    out.clear_output()\n",
    "    out1.clear_output()\n",
    "    out2.clear_output()\n",
    "    out3.clear_output()\n",
    "\n",
    "    ## calculating the auc\n",
    "    auc_ = round(roc_auc_score(tabl['target'], tabl['preds']),3)\n",
    "    \n",
    "    ## graphing auc roc curve\n",
    "    fpr, tpr, _ = roc_curve(tabl['target'],  tabl['preds'])\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(fpr, \n",
    "             tpr, \n",
    "             label=\"Model AUC={:.3f}\".format(auc_))\n",
    "    \n",
    "    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
    "\n",
    "    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "    \n",
    "    plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "    plt.legend(prop={'size':13}, loc='lower right')\n",
    "    \n",
    "    with out: plt.show()\n",
    "    lbl_pred.value = f'AUC of the model {auc_}'\n",
    "    \n",
    "    ## calculating F-scores and corresponding threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(tabl['target']\n",
    "                                                           , tabl['preds'])\n",
    "    ## F-1\n",
    "    f1_scores = 2*recall*precision/(recall+precision)\n",
    "    best_f1 = round(np.max(f1_scores),3)\n",
    "    thr_f1 = round(thresholds[np.argmax(f1_scores)],3)\n",
    "    cnfus1 = pd.crosstab(tabl['target']\n",
    "                         , (tabl['preds']>=thr_f1).astype(int)\n",
    "                         , rownames = ['Target']\n",
    "                         , colnames = ['Predicted'])\n",
    "    \n",
    "    sns.heatmap(cnfus1, annot=True\n",
    "                , cbar = False, fmt = 'd'\n",
    "                , annot_kws={\"size\": 24})\n",
    "    with out1: plt.show()\n",
    "    lbl_pred1.value = f'Best F1 Score: {best_f1}'\n",
    "    lbl_pred1Tr.value = f'@ Threshold: {thr_f1}'\n",
    "\n",
    "    \n",
    "    ## F-2\n",
    "    f2_scores = 5*recall*precision/(recall+(4*precision))\n",
    "    best_f2 = round(np.max(f2_scores),3)\n",
    "    thr_f2 = round(thresholds[np.argmax(f2_scores)],3)\n",
    "    cnfus2 = pd.crosstab(tabl['target']\n",
    "                         , (tabl['preds']>=thr_f2).astype(int)\n",
    "                         , rownames = ['Target']\n",
    "                         , colnames = ['Predicted'])\n",
    "    \n",
    "    sns.heatmap(cnfus2, annot=True\n",
    "                , cbar = False, fmt = 'd'\n",
    "                , annot_kws={\"size\": 24})\n",
    "    with out2: plt.show()\n",
    "    lbl_pred2.value = f'Best F2 Score: {best_f2}'\n",
    "    lbl_pred2Tr.value = f'@ Threshold: {thr_f2}'\n",
    "\n",
    "    \n",
    "    ## F-0.5\n",
    "    fPoint5_scores = 1.25*recall*precision/(recall+(0.25*precision))\n",
    "    best_fPoint5 = round(np.max(fPoint5_scores),3)\n",
    "    thr_fPoint5 = round(thresholds[np.argmax(fPoint5_scores)],3)\n",
    "    cnfusPoint5 = pd.crosstab(tabl['target']\n",
    "                              , (tabl['preds']>=thr_fPoint5).astype(int)\n",
    "                              , rownames = ['Target']\n",
    "                              , colnames = ['Predicted'])\n",
    "    \n",
    "    sns.heatmap(cnfusPoint5, annot=True\n",
    "                , cbar = False, fmt = 'd'\n",
    "                , annot_kws={\"size\": 24})\n",
    "    with out3: plt.show()\n",
    "    lbl_predPoint5.value = f'Best F0.5 Score: {best_fPoint5}'\n",
    "    lbl_predPoint5Tr.value = f'@ Threshold: {thr_fPoint5}'\n",
    "\n",
    "def chart(change):\n",
    "    tabl = pd.read_csv(io.BytesIO(btn_upload.data[-1]))\n",
    "    \n",
    "    out5.clear_output()\n",
    "    \n",
    "    ## ntile V target rate chart \n",
    "    x, y = true_ntile_dist(tabl, ntile_sldr.value)\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    g = sns.barplot(x = 'N-tile buckets'\n",
    "                    , y = 'Target Rate'\n",
    "                    , data = x)\n",
    "\n",
    "    g.axes.yaxis.set_major_formatter(PercentFormatter(1)) \n",
    "    # Show the plot\n",
    "    with out5: plt.show()\n",
    "        \n",
    "def response(change):\n",
    "    \n",
    "    tabl = pd.read_csv(io.BytesIO(btn_upload.data[-1]))\n",
    "    \n",
    "    out4.clear_output()\n",
    "\n",
    "    ## user defined threshold\n",
    "    preds_user_thresh = (tabl['preds']>=sldr.value).astype(int)\n",
    "    precision_user = round(precision_score(tabl['target']\n",
    "                                           , preds_user_thresh\n",
    "                                           , average = 'binary'),3)\n",
    "    recall_user = round(recall_score(tabl['target']\n",
    "                                     , preds_user_thresh\n",
    "                                     , average = 'binary'),3)\n",
    "    cnfususer = pd.crosstab(tabl['target']\n",
    "                            , preds_user_thresh\n",
    "                            , rownames = ['Target']\n",
    "                            , colnames = ['Predicted'])\n",
    "    lbl_user_precision.value = f'Precision: {precision_user}'\n",
    "    lbl_user_recall.value = f'Recall: {recall_user}'\n",
    "    sns.heatmap(cnfususer, annot=True\n",
    "                , cbar = False, fmt = 'd'\n",
    "                , annot_kws={\"size\": 24})\n",
    "    with out4: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload.observe(on_click, names=['data'])\n",
    "sldr.observe(response, names ='value')\n",
    "ntile_sldr.observe(chart, names ='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_box = VBox([lbl_pred1\n",
    "                 , lbl_pred1Tr\n",
    "                 , out1])\n",
    "left_box.layout.align_items = 'center'\n",
    "    \n",
    "centr_box = VBox([lbl_pred2\n",
    "                  , lbl_pred2Tr\n",
    "                  , out2])\n",
    "centr_box.layout.align_items = 'center'\n",
    "   \n",
    "right_box = VBox([lbl_predPoint5\n",
    "                  , lbl_predPoint5Tr\n",
    "                  , out3])\n",
    "right_box.layout.align_items = 'center'\n",
    "\n",
    "row1_right_box = VBox([out,lbl_pred]) \n",
    "row1_right_box.layout.align_items = 'center'\n",
    "\n",
    "row1_left_box = VBox([out5,ntile_sldr])\n",
    "row1_left_box.layout.align_items = 'center'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56d47631e9649d6bcaa166a4f8726e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Select your dataset'), FileUpload(value={}, accept='text/csv', description='Upload…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d62e03184bd4789acd0001338f2e1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Label(value=''), Label(value=''), Output()), layout=Layout(align_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##organize widgets\n",
    "vb = VBox([widgets.Label('Select your dataset')\n",
    "           , btn_upload\n",
    "           , HBox([row1_right_box,row1_left_box])] \n",
    "           , layout=Layout(border='solid'\n",
    "                           , width='80%'\n",
    "                           , margin='auto'))\n",
    "\n",
    "vb3 = VBox([HBox([left_box, centr_box, right_box])]\n",
    "                  ,layout=Layout(border='solid'\n",
    "                                 , width='80%'\n",
    "                                 , margin='auto'))\n",
    "           \n",
    "vb.layout.align_items = 'center'\n",
    "vb3.layout.align_items = 'center'\n",
    "display(vb)\n",
    "display(vb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a425274980674cd3856849ecbfb92e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Select the prediction Threshold'), FloatSlider(value=0.5, description='Threshold:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vb2 = VBox([widgets.Label('Select the prediction Threshold')\n",
    "            , sldr \n",
    "            , lbl_user_precision\n",
    "            , lbl_user_recall\n",
    "            , out4])\n",
    "vb2.layout.align_items = 'center'\n",
    "display(vb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "Precision = $\\frac{TP}{TP + FP}$\t\n",
    "\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "F-Score = 2 . $\\frac{precision . recall}{precision + recall}$\n",
    "\n",
    "F $\\beta$-Score = (1+$\\beta^{2}$) . $\\frac{precision . recall}{ \\beta^{2}.precision + recall}$\n",
    "\n",
    "Author - **Prabhat R.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
